{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "87091db6",
      "metadata": {},
      "source": [
        "# Week 2: KNN Practice Questions\n",
        "\n",
        "**Course:** COMP SCI 465 — Machine Learning (Spring 2026)  \n",
        "**Topic:** K-Nearest Neighbors (Classification & Regression)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d1ef981",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Q1 — KNN for Regression\n",
        "\n",
        "> A real estate company uses KNN to predict the **exact selling price** of a house based on nearby houses. What type of problem is this?\n",
        "\n",
        "**Answer:** **Regression** — The target variable (selling price) is a continuous value, so predicting it is a regression task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75be1bfb",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Q2 — KNN for Classification\n",
        "\n",
        "> A hospital uses KNN to predict whether a patient **has diabetes or not**. This is an example of:\n",
        "\n",
        "**Answer:** **Classification** — The output is one of two discrete categories (diabetes / no diabetes)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb8835a1",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Q3 — Majority Voting in KNN Classification\n",
        "\n",
        "> A KNN classifier uses **K = 5**. Among the nearest neighbors, **3 belong to Class A** and **2 belong to Class B**. What is the predicted class?\n",
        "\n",
        "**Answer:** **Class A** — KNN classification uses majority voting; the class with the most neighbors wins (3 > 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588e626b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Graph — KNN Classification: Majority Voting (K = 5)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "\n",
        "# Query point\n",
        "query = np.array([5.0, 5.0])\n",
        "\n",
        "# Neighbors: 3 Class A, 2 Class B\n",
        "np.random.seed(42)\n",
        "neighbors_a = np.array([[4.2, 5.8], [5.6, 5.5], [4.8, 4.2]])\n",
        "neighbors_b = np.array([[5.8, 4.3], [3.9, 4.6]])\n",
        "\n",
        "# Background scatter (extra training points)\n",
        "bg_a = np.array([[2, 7], [3, 8], [1.5, 6], [3.5, 6.5], [2.5, 5.5], [1, 8]])\n",
        "bg_b = np.array([[7, 2], [8, 3], [7.5, 1.5], [6.5, 2.5], [8.5, 1], [9, 3.5]])\n",
        "\n",
        "ax.scatter(bg_a[:, 0], bg_a[:, 1], c='#3b82f6', s=60, alpha=0.3, marker='o')\n",
        "ax.scatter(bg_b[:, 0], bg_b[:, 1], c='#ef4444', s=60, alpha=0.3, marker='s')\n",
        "\n",
        "# Draw circle around query\n",
        "circle = plt.Circle(query, 2.0, fill=False, linestyle='--', color='gray', linewidth=1.5, label='K=5 neighborhood')\n",
        "ax.add_patch(circle)\n",
        "\n",
        "# Plot neighbors\n",
        "ax.scatter(neighbors_a[:, 0], neighbors_a[:, 1], c='#3b82f6', s=150, marker='o', edgecolors='black', linewidths=1.5, zorder=5)\n",
        "ax.scatter(neighbors_b[:, 0], neighbors_b[:, 1], c='#ef4444', s=150, marker='s', edgecolors='black', linewidths=1.5, zorder=5)\n",
        "\n",
        "# Draw lines from query to each neighbor\n",
        "for pt in np.vstack([neighbors_a, neighbors_b]):\n",
        "    ax.plot([query[0], pt[0]], [query[1], pt[1]], 'k--', alpha=0.3, linewidth=1)\n",
        "\n",
        "# Query point\n",
        "ax.scatter(*query, c='#fbbf24', s=300, marker='*', edgecolors='black', linewidths=2, zorder=10, label='Query point')\n",
        "\n",
        "# Annotations\n",
        "ax.annotate('Query', query, textcoords=\"offset points\", xytext=(12, 12),\n",
        "            fontsize=11, fontweight='bold', color='#92400e')\n",
        "\n",
        "# Vote tally box\n",
        "props = dict(boxstyle='round,pad=0.5', facecolor='#f0fdf4', edgecolor='#16a34a', linewidth=2)\n",
        "ax.text(0.98, 0.98, 'Vote Tally\\n━━━━━━━━━\\nClass A: 3 votes ✓\\nClass B: 2 votes\\n\\nPrediction: Class A',\n",
        "        transform=ax.transAxes, fontsize=11, verticalalignment='top', horizontalalignment='right',\n",
        "        bbox=props, fontfamily='monospace')\n",
        "\n",
        "# Legend\n",
        "patch_a = mpatches.Patch(color='#3b82f6', label='Class A')\n",
        "patch_b = mpatches.Patch(color='#ef4444', label='Class B')\n",
        "ax.legend(handles=[patch_a, patch_b, \n",
        "                   plt.Line2D([0], [0], marker='*', color='w', markerfacecolor='#fbbf24',\n",
        "                              markersize=15, markeredgecolor='black', label='Query point')],\n",
        "          loc='lower left', fontsize=10, framealpha=0.9)\n",
        "\n",
        "ax.set_title('KNN Classification — Majority Voting (K = 5)', fontsize=14, fontweight='bold', pad=15)\n",
        "ax.set_xlabel('Feature 1', fontsize=11)\n",
        "ax.set_ylabel('Feature 2', fontsize=11)\n",
        "ax.set_xlim(0, 10)\n",
        "ax.set_ylim(0, 10)\n",
        "ax.set_aspect('equal')\n",
        "ax.grid(True, alpha=0.2)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef2d0c08",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Q4 — KNN Regression: Averaging Neighbors\n",
        "\n",
        "> A KNN regression model uses **K = 4** and the nearest neighbor values are: **10, 12, 14, 16**. What is the predicted value?\n",
        "\n",
        "**Answer:** **13** — KNN regression averages the target values of the K nearest neighbors:  \n",
        "$$\\hat{y} = \\frac{10 + 12 + 14 + 16}{4} = \\frac{52}{4} = 13$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884bfff5",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Graph — KNN Regression: Averaging K = 4 Neighbors\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "# Neighbor values\n",
        "neighbors = [10, 12, 14, 16]\n",
        "labels = ['Neighbor 1\\n(d = 1.2)', 'Neighbor 2\\n(d = 1.8)', 'Neighbor 3\\n(d = 2.1)', 'Neighbor 4\\n(d = 2.5)']\n",
        "colors = ['#60a5fa', '#34d399', '#fbbf24', '#f87171']\n",
        "\n",
        "bars = ax.bar(labels, neighbors, color=colors, edgecolor='black', linewidth=1.2, width=0.6, zorder=3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, val in zip(bars, neighbors):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
        "            f'{val}', ha='center', va='bottom', fontsize=13, fontweight='bold')\n",
        "\n",
        "# Average line\n",
        "avg = np.mean(neighbors)\n",
        "ax.axhline(y=avg, color='#dc2626', linewidth=2.5, linestyle='--', zorder=4, label=f'Prediction (avg) = {avg}')\n",
        "\n",
        "# Annotation for the average\n",
        "ax.annotate(f'Predicted Value = {avg}', xy=(3, avg), xytext=(2.5, avg + 2),\n",
        "            fontsize=12, fontweight='bold', color='#dc2626',\n",
        "            arrowprops=dict(arrowstyle='->', color='#dc2626', lw=2))\n",
        "\n",
        "ax.set_title('KNN Regression — Averaging K = 4 Nearest Neighbors', fontsize=14, fontweight='bold', pad=15)\n",
        "ax.set_ylabel('Target Value (Price)', fontsize=11)\n",
        "ax.set_ylim(0, 20)\n",
        "ax.legend(fontsize=11, loc='upper left')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "ax.set_axisbelow(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5781ed1",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Q5 — Distance-Weighted KNN\n",
        "\n",
        "> A KNN model gives **equal importance to far and near neighbors**, reducing accuracy. Which improvement should be applied?\n",
        "\n",
        "**Answer:** **Distance-weighted KNN** — Instead of treating all K neighbors equally, weight each neighbor's vote by the **inverse of its distance** to the query point. Closer neighbors get more influence.\n",
        "\n",
        "$$w_i = \\frac{1}{d(x_q,\\, x_i)}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25cc9f8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Graph — Uniform vs Distance-Weighted KNN\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# ─── Shared data ───\n",
        "query = np.array([0, 0])\n",
        "neighbors = np.array([[-0.5, 0.8], [0.3, -0.6], [-0.7, -0.3], [1.5, 1.2], [1.8, -0.5]])\n",
        "classes = ['A', 'A', 'B', 'B', 'B']\n",
        "class_colors = {'A': '#3b82f6', 'B': '#ef4444'}\n",
        "distances = np.sqrt(np.sum((neighbors - query)**2, axis=1))\n",
        "\n",
        "# ─── Panel 1: Uniform Weights ───\n",
        "ax = axes[0]\n",
        "for i, (pt, cls) in enumerate(zip(neighbors, classes)):\n",
        "    ax.scatter(*pt, c=class_colors[cls], s=180, edgecolors='black', linewidths=1.5, zorder=5,\n",
        "               marker='o' if cls == 'A' else 's')\n",
        "    ax.annotate(f'{cls}\\nw = 1', pt, textcoords=\"offset points\", xytext=(12, 8),\n",
        "                fontsize=9, fontweight='bold', color=class_colors[cls])\n",
        "    ax.plot([0, pt[0]], [0, pt[1]], 'k--', alpha=0.25)\n",
        "\n",
        "ax.scatter(*query, c='#fbbf24', s=250, marker='*', edgecolors='black', linewidths=2, zorder=10)\n",
        "ax.set_title('Uniform Weights', fontsize=13, fontweight='bold')\n",
        "props = dict(boxstyle='round,pad=0.4', facecolor='#fef3c7', edgecolor='#f59e0b', linewidth=1.5)\n",
        "ax.text(0.02, 0.02, 'A: 2 votes   B: 3 votes\\nPrediction: B', transform=ax.transAxes,\n",
        "        fontsize=10, va='bottom', bbox=props, fontfamily='monospace')\n",
        "ax.set_xlim(-2, 2.5)\n",
        "ax.set_ylim(-1.5, 2)\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "ax.grid(True, alpha=0.2)\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "# ─── Panel 2: Distance Weights ───\n",
        "ax = axes[1]\n",
        "weights = 1.0 / distances\n",
        "norm_weights = weights / weights.sum()\n",
        "\n",
        "for i, (pt, cls) in enumerate(zip(neighbors, classes)):\n",
        "    size = 100 + norm_weights[i] * 800\n",
        "    ax.scatter(*pt, c=class_colors[cls], s=size, edgecolors='black', linewidths=1.5, zorder=5,\n",
        "               marker='o' if cls == 'A' else 's', alpha=0.8)\n",
        "    ax.annotate(f'{cls}\\nw = {norm_weights[i]:.2f}', pt, textcoords=\"offset points\", xytext=(12, 8),\n",
        "                fontsize=9, fontweight='bold', color=class_colors[cls])\n",
        "    ax.plot([0, pt[0]], [0, pt[1]], 'k--', alpha=0.25)\n",
        "\n",
        "ax.scatter(*query, c='#fbbf24', s=250, marker='*', edgecolors='black', linewidths=2, zorder=10)\n",
        "ax.set_title('Distance Weights (1/d)', fontsize=13, fontweight='bold')\n",
        "\n",
        "w_a = sum(norm_weights[i] for i, c in enumerate(classes) if c == 'A')\n",
        "w_b = sum(norm_weights[i] for i, c in enumerate(classes) if c == 'B')\n",
        "winner = 'A' if w_a > w_b else 'B'\n",
        "props2 = dict(boxstyle='round,pad=0.4', facecolor='#f0fdf4', edgecolor='#16a34a', linewidth=1.5)\n",
        "ax.text(0.02, 0.02, f'A: {w_a:.2f}   B: {w_b:.2f}\\nPrediction: {winner}',\n",
        "        transform=ax.transAxes, fontsize=10, va='bottom', bbox=props2, fontfamily='monospace')\n",
        "ax.set_xlim(-2, 2.5)\n",
        "ax.set_ylim(-1.5, 2)\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "ax.grid(True, alpha=0.2)\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "fig.suptitle('Uniform vs Distance-Weighted KNN (K = 5)', fontsize=15, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "687e143b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Q6 — Why KNN Is Slow on Large Datasets\n",
        "\n",
        "> A KNN model becomes **very slow** when predicting on a large dataset. Why?\n",
        "\n",
        "**Answer:** KNN requires computing the **distance between the query point and every single training point** at prediction time. There is no pre-built model — it is a **lazy learner**. As the dataset grows, prediction cost grows linearly (or worse).\n",
        "\n",
        "| Phase | Lazy Learner (KNN) | Eager Learner (e.g., Logistic Regression) |\n",
        "|---|---|---|\n",
        "| **Training** | Almost instant (just stores data) | Slow (learns parameters) |\n",
        "| **Prediction** | Slow (computes all distances) | Fast (applies learned formula) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f28be5e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Graph — KNN Prediction Time vs Dataset Size\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "n_samples = np.array([100, 500, 1000, 5000, 10000, 50000, 100000])\n",
        "\n",
        "# Simulated prediction times (proportional to n for KNN, constant for eager)\n",
        "knn_time = n_samples * 0.0005      # grows linearly\n",
        "eager_time = np.full_like(n_samples, 0.5, dtype=float)  # roughly constant\n",
        "\n",
        "ax.plot(n_samples, knn_time, 'o-', color='#ef4444', linewidth=2.5, markersize=8, label='KNN (lazy learner)', zorder=5)\n",
        "ax.plot(n_samples, eager_time, 's--', color='#3b82f6', linewidth=2.5, markersize=8, label='Logistic Regression (eager)', zorder=5)\n",
        "\n",
        "ax.fill_between(n_samples, knn_time, eager_time, where=(knn_time > eager_time),\n",
        "                alpha=0.08, color='#ef4444')\n",
        "\n",
        "ax.set_xlabel('Number of Training Samples', fontsize=12)\n",
        "ax.set_ylabel('Prediction Time (ms) — illustrative', fontsize=12)\n",
        "ax.set_title('KNN Prediction Cost Grows with Dataset Size', fontsize=14, fontweight='bold', pad=15)\n",
        "ax.legend(fontsize=11, loc='upper left')\n",
        "ax.set_xscale('log')\n",
        "ax.set_yscale('log')\n",
        "ax.grid(True, alpha=0.3, which='both')\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "# Annotation\n",
        "ax.annotate('KNN must compute distance\\nto every training point',\n",
        "            xy=(50000, 50000*0.0005), xytext=(3000, 40),\n",
        "            fontsize=10, color='#b91c1c', fontweight='bold',\n",
        "            arrowprops=dict(arrowstyle='->', color='#b91c1c', lw=1.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a40547",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Q7 — Adjusting K for Fraud Detection\n",
        "\n",
        "> A fraud detection KNN model predicts many **legitimate transactions as fraud** (high false-positive rate). Which K-related adjustment may help?\n",
        "\n",
        "**Answer:** **Increase K.** A small K makes the model sensitive to noise — individual outliers can dominate the vote. Increasing K smooths the decision boundary and reduces false positives by requiring a stronger consensus among neighbors.\n",
        "\n",
        "| K Value | Behavior |\n",
        "|---|---|\n",
        "| Small K (e.g., 1–3) | Captures local patterns but prone to noise / overfitting |\n",
        "| Large K (e.g., 15–25) | Smoother boundaries, more stable, but may underfit |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64452367",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Graph — Effect of K on Decision Boundary (Overfitting vs Underfitting)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Generate a noisy 2-class dataset\n",
        "X, y = make_moons(n_samples=200, noise=0.3, random_state=42)\n",
        "\n",
        "k_values = [1, 5, 25]\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "for ax, k in zip(axes, k_values):\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    # Decision boundary\n",
        "    xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 300),\n",
        "                         np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 300))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.15, cmap='RdBu', levels=1)\n",
        "    ax.contour(xx, yy, Z, colors='gray', linewidths=1.5, levels=1, linestyles='--')\n",
        "\n",
        "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='#3b82f6', edgecolors='black', s=40, label='Class 0')\n",
        "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='#ef4444', edgecolors='black', s=40, label='Class 1')\n",
        "\n",
        "    train_acc = clf.score(X, y)\n",
        "    status = 'Overfitting' if k == 1 else ('Good balance' if k == 5 else 'Underfitting')\n",
        "    ax.set_title(f'K = {k}  ({status})\\nTrain Acc: {train_acc:.2f}', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "    ax.legend(fontsize=9, loc='lower left')\n",
        "    ax.grid(True, alpha=0.15)\n",
        "\n",
        "fig.suptitle('Effect of K on KNN Decision Boundary', fontsize=15, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a8d536e",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "| Concept | Summary |\n",
        "|---|---|\n",
        "| **Classification vs Regression** | Discrete labels = classification; continuous values = regression |\n",
        "| **Majority Voting** | KNN classification picks the class with the most neighbors |\n",
        "| **Averaging** | KNN regression averages the target values of K neighbors |\n",
        "| **Distance Weighting** | Closer neighbors get higher influence via $w = 1/d$ |\n",
        "| **Lazy Learner** | KNN stores training data and computes distances at prediction time |\n",
        "| **Choosing K** | Small K = overfitting / noise-sensitive; Large K = smoother / possible underfitting |"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
